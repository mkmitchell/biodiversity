{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f694c237-76a6-43d3-b5c4-d3b27a84256e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 1 written.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "\n",
    "input_path = '/mnt/e/backupfrompc/ebd_US_relSep-2025.txt'\n",
    "\n",
    "output_dir = \"/mnt/d/ebirdpyarrow\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "rows_per_shard = 5_000_000\n",
    "shard_index = 0\n",
    "\n",
    "# Read in chunks\n",
    "df_iter = pd.read_csv(input_path, sep=\"\\t\", quoting=1, on_bad_lines='skip', low_memory=False, chunksize=rows_per_shard)\n",
    "\n",
    "for df in df_iter:\n",
    "    # Normalize column names\n",
    "    lowercase_names = [name.lower().strip().replace(' ', '_') for name in df.columns]\n",
    "    df.columns = lowercase_names\n",
    "    df['scientific_name'] = df['scientific_name'].str.strip().str.lower().str.replace(' ', '_')\n",
    "    # Convert to PyArrow Table\n",
    "    table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "\n",
    "    # Write to partitioned dataset\n",
    "    pq.write_to_dataset(\n",
    "        table,\n",
    "        root_path=output_dir,\n",
    "        partition_cols=['scientific_name'],\n",
    "        compression='snappy'\n",
    "    )\n",
    "\n",
    "    shard_index += 1\n",
    "    print(f\"Shard {shard_index} written.\")\n",
    "\n",
    "print(\"Partitioned Parquet dataset created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b49fb74-12f8-47a3-8eb0-d6d45edcc202",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_657/2108307660.py:19: PerformanceWarning: Determining the column names of a LazyFrame requires resolving its schema, which is a potentially expensive operation. Use `LazyFrame.collect_schema().names()` to get the column names without this warning.\n",
      "  df.rename({col: col.lower().strip().replace(' ', '_') for col in df.columns})\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "\n",
    "input_path = '/mnt/e/backupfrompc/ebd_US_relSep-2025.txt'\n",
    "output_dir = \"/mnt/d/ebirdpolars\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Lazy scan\n",
    "df = pl.scan_csv(\n",
    "    input_path,\n",
    "    separator=\"\\t\",\n",
    "    quote_char='\"',\n",
    "    ignore_errors=True,\n",
    "    low_memory=True\n",
    ")\n",
    "\n",
    "# Transformations (still lazy)\n",
    "df = (\n",
    "    df.rename({col: col.lower().strip().replace(' ', '_') for col in df.columns})\n",
    "      .with_columns(\n",
    "          pl.col(\"scientific_name\")\n",
    "          .str.strip_chars()\n",
    "          .str.to_lowercase()\n",
    "          .str.replace(\" \", \"_\")\n",
    "      )\n",
    ")\n",
    "\n",
    "# Streaming write to Parquet\n",
    "df.lazy().sink_parquet(\n",
    "    pl.PartitionByKey(output_dir, by=[\"scientific_name\"]),\n",
    "    mkdir=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df7d2b5a-35ab-42b1-94f6-11d676f6512e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting species: Junco hyemalis\n"
     ]
    },
    {
     "ename": "ConversionException",
     "evalue": "Conversion Error: CSV Error on Line: 193802268\nOriginal Line: URN:CornellLabOfOrnithology:EBIRD:OBS506530015\t2025-10-08 16:48:13.391954\t33576\tspecies\tavibase-8BBB3255\tCommon Grackle\tQuiscalus quiscula\t\t\t\t2\t\t\t\t\tUnited States\tUS\tMaine\tUS-ME\tSagadahoc\tUS-ME-023\t\t\t\t43069F7NE\tPopham / Seawall ISS Area\tL977151\tH\t43.7327784\t-69.8036957\t2017-05-30\t17:40:00\tobsr43189\t\tS37276076\tTraveling\tTraveling\tP22\tMaine Bird Atlas|International Shorebird Survey (ISS)\t1003|1039\t75\t3.219\t\t1\t1\t\t0\t1\t0\t\t\t\nError when converting column \"PROJECT IDENTIFIERS\". Could not convert string \"1003|1039\" to 'BIGINT'\n\nColumn PROJECT IDENTIFIERS is being converted as type BIGINT\nThis type was auto-detected from the CSV file.\nPossible solutions:\n* Override the type for this column manually by setting the type explicitly, e.g., types={'PROJECT IDENTIFIERS': 'VARCHAR'}\n* Set the sample size to a larger value to enable the auto-detection to scan more values, e.g., sample_size=-1\n* Use a COPY statement to automatically derive types from an existing table.\n* Check whether the null string value is set correctly (e.g., nullstr = 'N/A')\n\n  file = /mnt/e/backupfrompc/ebd_US_relSep-2025.txt\n  delimiter = \t (Set By User)\n  quote = \" (Auto-Detected)\n  escape = \" (Auto-Detected)\n  new_line = \\n (Auto-Detected)\n  header = true (Set By User)\n  skip_rows = 0 (Auto-Detected)\n  comment = (empty) (Auto-Detected)\n  strict_mode = true (Auto-Detected)\n  date_format =  (Auto-Detected)\n  timestamp_format =  (Auto-Detected)\n  null_padding = 0\n  sample_size = 20480\n  ignore_errors = false\n  all_varchar = 0\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConversionException\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     35\u001b[39m sql = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[33mCOPY (\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[33m    SELECT \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns_sql\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     41\u001b[39m \u001b[33mTO \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/scientific_name=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msafe_sp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m (FORMAT \u001b[39m\u001b[33m'\u001b[39m\u001b[33mparquet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, COMPRESSION \u001b[39m\u001b[33m'\u001b[39m\u001b[33msnappy\u001b[39m\u001b[33m'\u001b[39m\u001b[33m);\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExporting species: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mConversionException\u001b[39m: Conversion Error: CSV Error on Line: 193802268\nOriginal Line: URN:CornellLabOfOrnithology:EBIRD:OBS506530015\t2025-10-08 16:48:13.391954\t33576\tspecies\tavibase-8BBB3255\tCommon Grackle\tQuiscalus quiscula\t\t\t\t2\t\t\t\t\tUnited States\tUS\tMaine\tUS-ME\tSagadahoc\tUS-ME-023\t\t\t\t43069F7NE\tPopham / Seawall ISS Area\tL977151\tH\t43.7327784\t-69.8036957\t2017-05-30\t17:40:00\tobsr43189\t\tS37276076\tTraveling\tTraveling\tP22\tMaine Bird Atlas|International Shorebird Survey (ISS)\t1003|1039\t75\t3.219\t\t1\t1\t\t0\t1\t0\t\t\t\nError when converting column \"PROJECT IDENTIFIERS\". Could not convert string \"1003|1039\" to 'BIGINT'\n\nColumn PROJECT IDENTIFIERS is being converted as type BIGINT\nThis type was auto-detected from the CSV file.\nPossible solutions:\n* Override the type for this column manually by setting the type explicitly, e.g., types={'PROJECT IDENTIFIERS': 'VARCHAR'}\n* Set the sample size to a larger value to enable the auto-detection to scan more values, e.g., sample_size=-1\n* Use a COPY statement to automatically derive types from an existing table.\n* Check whether the null string value is set correctly (e.g., nullstr = 'N/A')\n\n  file = /mnt/e/backupfrompc/ebd_US_relSep-2025.txt\n  delimiter = \t (Set By User)\n  quote = \" (Auto-Detected)\n  escape = \" (Auto-Detected)\n  new_line = \\n (Auto-Detected)\n  header = true (Set By User)\n  skip_rows = 0 (Auto-Detected)\n  comment = (empty) (Auto-Detected)\n  strict_mode = true (Auto-Detected)\n  date_format =  (Auto-Detected)\n  timestamp_format =  (Auto-Detected)\n  null_padding = 0\n  sample_size = 20480\n  ignore_errors = false\n  all_varchar = 0\n\n"
     ]
    }
   ],
   "source": [
    "import duckdb, re\n",
    "\n",
    "csv_path = '/mnt/e/backupfrompc/ebd_US_relSep-2025.txt'\n",
    "output_path = '/mnt/d/ebirdduckdb'\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# Get column names without loading data\n",
    "cols = con.execute(f\"\"\"\n",
    "    SELECT * FROM read_csv_auto('{csv_path}', header=True, delim='\\t') LIMIT 0\n",
    "\"\"\").fetchdf().columns.tolist()\n",
    "\n",
    "# Sanitize column names for aliases\n",
    "select_expr = []\n",
    "for col in cols:\n",
    "    new_name = re.sub(r'[^a-z0-9]+', '_', col.strip().lower())\n",
    "    quoted_col = f'\"{col}\"'\n",
    "    if col.lower() == \"scientific_name\":\n",
    "        select_expr.append(f\"lower(replace(trim({quoted_col}), ' ', '_')) AS {new_name}\")\n",
    "    else:\n",
    "        select_expr.append(f\"{quoted_col} AS {new_name}\")\n",
    "\n",
    "# Join into a single string for SELECT\n",
    "columns_sql = \", \".join(select_expr)\n",
    "\n",
    "# Get distinct species from original column name\n",
    "species_list = con.execute(f\"\"\"\n",
    "    SELECT DISTINCT \"SCIENTIFIC NAME\"\n",
    "    FROM read_csv_auto('{csv_path}', header=True, delim='\\t')\n",
    "\"\"\").fetchdf()[\"SCIENTIFIC NAME\"].dropna().tolist()\n",
    "\n",
    "# Export each species separately to avoid OOM\n",
    "for sp in species_list:\n",
    "    safe_sp = sp.replace(\" \", \"_\").lower()\n",
    "    sql = f\"\"\"\n",
    "    COPY (\n",
    "        SELECT {columns_sql}\n",
    "        FROM read_csv_auto('{csv_path}', header=True, delim='\\t')\n",
    "        WHERE scientific_name = '{sp}'\n",
    "    )\n",
    "    TO '{output_path}/scientific_name={safe_sp}' (FORMAT 'parquet', COMPRESSION 'snappy');\n",
    "    \"\"\"\n",
    "    print(f\"Exporting species: {sp}\")\n",
    "    con.execute(sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168320f7-36e5-4c55-97a4-0f6ba3a067a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
