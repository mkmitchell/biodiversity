{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62a89537-3292-4108-a199-9372aecf4036",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ee\n",
    "import os\n",
    "import glob\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "ee.Authenticate()\n",
    "\n",
    "# Authenticate & initialize Earth Engine\n",
    "ee.Initialize(project='biodiversity-478015')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1509d72-e703-44ef-a91d-e660bb8a8748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "csv_folder = \"/mnt/c/gbif\"\n",
    "parquet_folder = \"/mnt/c/ebirdpolars\"\n",
    "\n",
    "asset_folder = \"/projects/gbif\"\n",
    "aoifile = '/mnt/e/gis/BaseData/MAV_Boundary_4326_wkb.parquet'\n",
    "\n",
    "scale = 100  # Adjust based on your imagery resolution\n",
    "aoi_gdf = gpd.read_parquet(aoifile)\n",
    "aoi_geom = ee.Geometry.Polygon(list(aoi_gdf.geometry.union_all().exterior.coords))\n",
    "# Buffer by 5000 m for focal stats\n",
    "aoi_buffered = aoi_geom.buffer(5000)  # 5 km buffer\n",
    "\n",
    "species_list = [\n",
    "    \"Protonotaria citrea\",\n",
    "    \"Limnothlypis swainsonii\",\n",
    "    \"Setophaga americana\",\n",
    "    \"Empidonax virescens\",\n",
    "    \"Coccyzus americanus\",\n",
    "    \"Vireo griseus\",\n",
    "    \"Setophaga cerulea\",\n",
    "    \"Hylocichla mustelina\",\n",
    "    \"Parkesia motacilla\",\n",
    "    \"Geothlypis formosa\",\n",
    "    \"Archilochus colubris\",\n",
    "    \"Elanoides forficatus\",\n",
    "    \"Vireo flavifrons\",\n",
    "    \"Buteo lineatus\",\n",
    "    \"Setophaga dominica\",\n",
    "    \"Setophaga citrina\",\n",
    "    \"Dryocopus pileatus\",\n",
    "    \"Meleagris gallopavo\",\n",
    "    \"Sphyrapicus varius\",\n",
    "    \"Odocoileus virginianus\",     # white tailed deer\n",
    "    \"Ursus americanus\",           # Black bear\n",
    "    \"Anaxyrus americanus\",        # American Toad\n",
    "    \"Anaxyrus fowleri\",           # Fowler's Toad\n",
    "    \"Gastrophryne carolinensis\",  # Eastern Narrow-mouthed Toad\n",
    "    \"Hyla avivoca\",               # Bird-voiced Treefrog\n",
    "    \"Hyla chrysoscelis\",          # Cope's Gray Treefrog\n",
    "    \"Hyla cinerea\",               # Green Treefrog\n",
    "    \"Hyla squirella\",             # Squirrel Treefrog    \n",
    "    \"Hyla versicolor\",            # Gray Treefrog\n",
    "    \"Lithobates catesbeianus\",    # American Bullfrog\n",
    "    \"Lithobates clamitans\",       # Bronze Frog\n",
    "    \"Lithobates palustris\",       # Pickerel Frog\n",
    "    \"Lithobates sphenocephalus\",  # Southern Leopard Frog\n",
    "    \"Pseudacris crucifer\",        # Spring Peeper\n",
    "    \"Pseudacris fouquettei\",      # Cajun Chorus Frog\n",
    "    \"Kinosternon subrubrum\",      # Eastern Mud Turtle\n",
    "    \"Apalone spinifera\",          # Spiny Softshell Turtle   \n",
    "    \"Macrochelys temmincki\"       # Alligator Snapping Turtle    \n",
    "    ]\n",
    "species_list = [item.strip().lower().replace(' ', '_') for item in species_list]\n",
    "log_file = \"process_log.txt\"\n",
    "\n",
    "# -------------------------\n",
    "# Logging\n",
    "# -------------------------\n",
    "def log(message):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(f\"[{timestamp}] {message}\\n\")\n",
    "    print(message)\n",
    "\n",
    "# -------------------------\n",
    "# Load species data from CSV or DuckDB\n",
    "# -------------------------\n",
    "def load_species_data(species):\n",
    "    csv_files = glob.glob(os.path.join(csv_folder, f\"{species}*.csv\"))\n",
    "    df = None\n",
    "    if len(csv_files)>0:\n",
    "        print('Reading csv')\n",
    "        try:\n",
    "            # Read and combine all CSVs\n",
    "            df_list = [pd.read_csv(f) for f in csv_files]\n",
    "            df = pd.concat(df_list, ignore_index=True)\n",
    "            #df['date'] = pd.to_datetime(df['eventdate'])\n",
    "            df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "            df = df[['basisofrecord', 'species', 'latitude', 'longitude','coordinateuncertaintyinmeters', 'date']]\n",
    "            df = df.dropna(how='any')\n",
    "            df = df[df['coordinateuncertaintyinmeters'] <=100]\n",
    "            log(f\"✔ Loaded CSV for {species}\")\n",
    "        except Exception as e:\n",
    "            log(f\"⚠️ Failed to read CSV for {species}: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print('Reading parquet')\n",
    "        try:\n",
    "            query = f\"\"\"\n",
    "                    SELECT \n",
    "                        lower(eb.scientific_name) as scientific_name,\n",
    "                        eb.observation_date,\n",
    "                        eb.protocol_name,\n",
    "                        eb.effort_distance_km,\n",
    "                        eb.longitude,\n",
    "                        eb.latitude\n",
    "                    FROM read_parquet('{parquet_folder}/scientific_name={species}/*.parquet', hive_partitioning = true) AS eb\n",
    "                    JOIN aoi\n",
    "                        ON ST_Intersects(ST_Point(eb.longitude, eb.latitude), aoi.geometry)\n",
    "                    WHERE lower(eb.scientific_name) = '{species}'\n",
    "                      AND CAST(substr(eb.observation_date, 1, 4) AS INTEGER) BETWEEN 2017 AND 2024;  \n",
    "            \"\"\"\n",
    "            df = con.execute(query).fetchdf()\n",
    "            print(len(df))\n",
    "            df = df[(df['effort_distance_km']<.1) | (df['protocol_name'] == 'Stationary')]\n",
    "            df = df .drop(columns=['effort_distance_km'])\n",
    "            df['date'] = pd.to_datetime(df['observation_date'])\n",
    "            # Extract year, month, day\n",
    "            df['year'] = df['date'].dt.year\n",
    "            df['month'] = df['date'].dt.month\n",
    "            df['day'] = df['date'].dt.day   \n",
    "            if df.empty:\n",
    "                log(f\"❌ No matching records in Parquet for {species}\")\n",
    "                return None\n",
    "            log(f\"✔ Loaded Parquet data for {species}\")\n",
    "        except Exception as e:\n",
    "            log(f\"⚠️ DuckDB query failed for {species}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Filter by date range\n",
    "    try:\n",
    "        if df.empty:\n",
    "            log(f\"❌ No records in date range for {species}\")\n",
    "            return None\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        log(f\"⚠️ Failed to filter dates for {species}: {e}\")\n",
    "        return None\n",
    "\n",
    "# -------------------------\n",
    "# Convert DataFrame to EE FeatureCollection\n",
    "# -------------------------\n",
    "def df_to_ee_fc(df, datefield, lon_col='longitude', lat_col='latitude', properties=None):\n",
    "    df = df.dropna(subset=[lon_col, lat_col])\n",
    "    print('converting to fc')\n",
    "    if properties is None:\n",
    "        properties = [c for c in df.columns if c not in [lon_col, lat_col]]\n",
    "    features = []\n",
    "    for _, row in df.iterrows():\n",
    "        geom = ee.Geometry.Point([row[lon_col], row[lat_col]])\n",
    "        props = {k: row[k] for k in properties}\n",
    "        props['obs_date'] = row[datefield].strftime('%Y-%m-%d')\n",
    "        features.append(ee.Feature(geom, props))\n",
    "    return ee.FeatureCollection(features)\n",
    "    \n",
    "# -------------------------\n",
    "# Split FeatureCollection into subsets\n",
    "# -------------------------\n",
    "def split_fc(fc, n_subsets=10):\n",
    "    print('splitting fc')\n",
    "    n_points = fc.size().getInfo()\n",
    "    points_list = fc.toList(n_points)\n",
    "    subsets = []\n",
    "    step = n_points // n_subsets + 1\n",
    "    for i in range(0, n_points, step):\n",
    "        subset = ee.FeatureCollection(points_list.slice(i, i + step))\n",
    "        subsets.append(subset)\n",
    "    print('split')\n",
    "    return subsets\n",
    "\n",
    "# -------------------------\n",
    "# Dynamic World mode image\n",
    "# -------------------------\n",
    "def get_dw_mode_image(obs_date):\n",
    "    obs_date = ee.Date(obs_date)\n",
    "    start_date = obs_date.advance(-3, 'month')\n",
    "    dw_collection = ee.ImageCollection(\"GOOGLE/DYNAMICWORLD/V1\").select(\"label\")\n",
    "    dw_filtered = dw_collection.filterDate(start_date, obs_date)\n",
    "    return dw_filtered.reduce(ee.Reducer.mode())\n",
    "\n",
    "# -------------------------\n",
    "# Percent cover for DW classes\n",
    "# -------------------------\n",
    "def compute_dw_percent_cover(dw_img, radius_m):\n",
    "    class_ids = list(range(9))\n",
    "    kernel = ee.Kernel.circle(radius=radius_m, units='meters', normalize=True)\n",
    "    cover_images = []\n",
    "    for class_id in class_ids:\n",
    "        mask = dw_img.eq(class_id)\n",
    "        pct = mask.reduceNeighborhood(ee.Reducer.mean(), kernel).multiply(100).rename(f'dw_class_{class_id}_pct_{radius_m}m')\n",
    "        cover_images.append(pct)\n",
    "    return ee.Image.cat(cover_images)\n",
    "\n",
    "# -------------------------\n",
    "# Forest edge and core\n",
    "# -------------------------\n",
    "def compute_forest_metrics(dw_img, radius_m):\n",
    "    forest_mask = dw_img.eq(1)\n",
    "    non_forest_mask = dw_img.neq(1)\n",
    "\n",
    "    forest_edges = ee.Algorithms.CannyEdgeDetector(image=forest_mask, threshold=0.5, sigma=1)\n",
    "    edge_density = forest_edges.reduceNeighborhood(\n",
    "        ee.Reducer.sum(), ee.Kernel.circle(radius=radius_m, units='meters')\n",
    "    ).rename('forest_edge_length')\n",
    "    kernel = ee.Kernel.circle(radius=radius_m, units='meters', normalize=True)\n",
    "    non_forest_buffer = non_forest_mask.focal_max(radius=100, units='meters')\n",
    "    forest_core = forest_mask.And(non_forest_buffer.Not()).rename('forest_core')\n",
    "    pct = forest_core.eq(1).reduceNeighborhood(ee.Reducer.mean(), kernel).multiply(100).rename(f'forest_core_pct_{radius_m}')\n",
    "    return ee.Image.cat([edge_density.rename(f'forest_edge_{radius_m}'), pct])\n",
    "\n",
    "# -------------------------\n",
    "# Export Task Function\n",
    "# -------------------------\n",
    "def export_subset(sub_fc, species, subset_index):\n",
    "    print('Setting up exports')\n",
    "    def process_feature(f):\n",
    "        obs_date = f.get('obs_date')\n",
    "        dw_img = get_dw_mode_image(obs_date)\n",
    "\n",
    "        cover_100m = compute_dw_percent_cover(dw_img, radius_m=100)\n",
    "        cover_10km = compute_dw_percent_cover(dw_img, radius_m=10000)\n",
    "        forest_metrics_100m = compute_forest_metrics(dw_img, 100)\n",
    "        forest_metrics_10km = compute_forest_metrics(dw_img, 10000)\n",
    "\n",
    "        full_img = ee.Image.cat([cover_100m, cover_10km, forest_metrics_100m, forest_metrics_10km])\n",
    "        sampled = full_img.sampleRegions(\n",
    "            collection=ee.FeatureCollection([f]),\n",
    "            scale=100,\n",
    "            geometries=True,\n",
    "            tileScale=4\n",
    "        )\n",
    "        return sampled\n",
    "\n",
    "    try:\n",
    "        sampled_fc = sub_fc.map(process_feature).flatten()\n",
    "        export_desc = f\"{species}_subset{subset_index}\"\n",
    "        asset_id = f\"{asset_folder}/{export_desc}\"\n",
    "\n",
    "        task = ee.batch.Export.table.toDrive(\n",
    "            collection=sampled_fc,\n",
    "            description=export_desc,\n",
    "            fileNamePrefix=asset_id\n",
    "        )\n",
    "        task.start()\n",
    "        log(f\"✔ Export started for {species} subset {subset_index}\")\n",
    "    except Exception as e:\n",
    "        log(f\"❌ Export failed for {species} subset {subset_index}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc92cd19-b2c2-4f72-be4a-76a4af992eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing: protonotaria_citrea ---\n",
      "Reading parquet\n",
      "9467\n",
      "✔ Loaded Parquet data for protonotaria_citrea\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for protonotaria_citrea subset 3\n",
      "Setting up exports\n",
      "✔ Export started for protonotaria_citrea subset 1\n",
      "Setting up exports\n",
      "✔ Export started for protonotaria_citrea subset 5\n",
      "Setting up exports\n",
      "✔ Export started for protonotaria_citrea subset 2\n",
      "Setting up exports\n",
      "✔ Export started for protonotaria_citrea subset 0\n",
      "Setting up exports\n",
      "✔ Export started for protonotaria_citrea subset 4\n",
      "✔ Export started for protonotaria_citrea subset 7\n",
      "✔ Export started for protonotaria_citrea subset 6\n",
      "✔ Export started for protonotaria_citrea subset 8\n",
      "✔ Export started for protonotaria_citrea subset 9\n",
      "\n",
      "--- Processing: limnothlypis_swainsonii ---\n",
      "Reading parquet\n",
      "279\n",
      "✔ Loaded Parquet data for limnothlypis_swainsonii\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for limnothlypis_swainsonii subset 2\n",
      "Setting up exports\n",
      "✔ Export started for limnothlypis_swainsonii subset 0\n",
      "Setting up exports\n",
      "✔ Export started for limnothlypis_swainsonii subset 4\n",
      "✔ Export started for limnothlypis_swainsonii subset 3\n",
      "✔ Export started for limnothlypis_swainsonii subset 1\n",
      "✔ Export started for limnothlypis_swainsonii subset 5\n",
      "✔ Export started for limnothlypis_swainsonii subset 6\n",
      "\n",
      "--- Processing: setophaga_americana ---\n",
      "Reading parquet\n",
      "2934\n",
      "✔ Loaded Parquet data for setophaga_americana\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for setophaga_americana subset 2\n",
      "Setting up exports\n",
      "✔ Export started for setophaga_americana subset 0\n",
      "Setting up exports\n",
      "✔ Export started for setophaga_americana subset 3\n",
      "Setting up exports\n",
      "✔ Export started for setophaga_americana subset 4\n",
      "Setting up exports\n",
      "✔ Export started for setophaga_americana subset 1\n",
      "Setting up exports\n",
      "✔ Export started for setophaga_americana subset 6\n",
      "✔ Export started for setophaga_americana subset 5\n",
      "✔ Export started for setophaga_americana subset 7\n",
      "✔ Export started for setophaga_americana subset 8\n",
      "✔ Export started for setophaga_americana subset 9\n",
      "\n",
      "--- Processing: empidonax_virescens ---\n",
      "Reading parquet\n",
      "3609\n",
      "✔ Loaded Parquet data for empidonax_virescens\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for empidonax_virescens subset 1\n",
      "Setting up exports\n",
      "✔ Export started for empidonax_virescens subset 2\n",
      "Setting up exports\n",
      "✔ Export started for empidonax_virescens subset 0\n",
      "Setting up exports\n",
      "✔ Export started for empidonax_virescens subset 4\n",
      "Setting up exports\n",
      "✔ Export started for empidonax_virescens subset 5\n",
      "Setting up exports\n",
      "✔ Export started for empidonax_virescens subset 3\n",
      "✔ Export started for empidonax_virescens subset 6\n",
      "✔ Export started for empidonax_virescens subset 7\n",
      "✔ Export started for empidonax_virescens subset 9\n",
      "✔ Export started for empidonax_virescens subset 8\n",
      "\n",
      "--- Processing: coccyzus_americanus ---\n",
      "Reading parquet\n",
      "6666\n",
      "✔ Loaded Parquet data for coccyzus_americanus\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for coccyzus_americanus subset 3\n",
      "Setting up exports\n",
      "✔ Export started for coccyzus_americanus subset 1\n",
      "Setting up exports\n",
      "✔ Export started for coccyzus_americanus subset 4\n",
      "Setting up exports\n",
      "✔ Export started for coccyzus_americanus subset 2\n",
      "Setting up exports\n",
      "✔ Export started for coccyzus_americanus subset 0\n",
      "Setting up exports\n",
      "✔ Export started for coccyzus_americanus subset 5\n",
      "✔ Export started for coccyzus_americanus subset 7\n",
      "✔ Export started for coccyzus_americanus subset 6\n",
      "✔ Export started for coccyzus_americanus subset 8\n",
      "✔ Export started for coccyzus_americanus subset 9\n",
      "\n",
      "--- Processing: vireo_griseus ---\n",
      "Reading parquet\n",
      "9734\n",
      "✔ Loaded Parquet data for vireo_griseus\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for vireo_griseus subset 0\n",
      "Setting up exports\n",
      "✔ Export started for vireo_griseus subset 2\n",
      "Setting up exports\n",
      "✔ Export started for vireo_griseus subset 4\n",
      "Setting up exports\n",
      "✔ Export started for vireo_griseus subset 1\n",
      "Setting up exports\n",
      "✔ Export started for vireo_griseus subset 3\n",
      "Setting up exports\n",
      "✔ Export started for vireo_griseus subset 5\n",
      "✔ Export started for vireo_griseus subset 6\n",
      "✔ Export started for vireo_griseus subset 8\n",
      "✔ Export started for vireo_griseus subset 7\n",
      "✔ Export started for vireo_griseus subset 9\n",
      "\n",
      "--- Processing: setophaga_cerulea ---\n",
      "Reading parquet\n",
      "211\n",
      "✔ Loaded Parquet data for setophaga_cerulea\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for setophaga_cerulea subset 4\n",
      "Setting up exports\n",
      "✔ Export started for setophaga_cerulea subset 2\n",
      "Setting up exports\n",
      "✔ Export started for setophaga_cerulea subset 3\n",
      "✔ Export started for setophaga_cerulea subset 1\n",
      "✔ Export started for setophaga_cerulea subset 0\n",
      "✔ Export started for setophaga_cerulea subset 5\n",
      "✔ Export started for setophaga_cerulea subset 6\n",
      "\n",
      "--- Processing: hylocichla_mustelina ---\n",
      "Reading parquet\n",
      "1646\n",
      "✔ Loaded Parquet data for hylocichla_mustelina\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for hylocichla_mustelina subset 1\n",
      "Setting up exports\n",
      "✔ Export started for hylocichla_mustelina subset 3\n",
      "Setting up exports\n",
      "✔ Export started for hylocichla_mustelina subset 0\n",
      "Setting up exports\n",
      "✔ Export started for hylocichla_mustelina subset 4\n",
      "Setting up exports\n",
      "✔ Export started for hylocichla_mustelina subset 5\n",
      "Setting up exports\n",
      "✔ Export started for hylocichla_mustelina subset 6\n",
      "✔ Export started for hylocichla_mustelina subset 2\n",
      "✔ Export started for hylocichla_mustelina subset 7\n",
      "✔ Export started for hylocichla_mustelina subset 9\n",
      "✔ Export started for hylocichla_mustelina subset 8\n",
      "\n",
      "--- Processing: parkesia_motacilla ---\n",
      "Reading parquet\n",
      "671\n",
      "✔ Loaded Parquet data for parkesia_motacilla\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for parkesia_motacilla subset 1\n",
      "Setting up exports\n",
      "✔ Export started for parkesia_motacilla subset 0\n",
      "Setting up exports\n",
      "✔ Export started for parkesia_motacilla subset 3\n",
      "Setting up exports\n",
      "✔ Export started for parkesia_motacilla subset 2\n",
      "Setting up exports\n",
      "✔ Export started for parkesia_motacilla subset 5\n",
      "✔ Export started for parkesia_motacilla subset 6\n",
      "✔ Export started for parkesia_motacilla subset 8\n",
      "✔ Export started for parkesia_motacilla subset 4\n",
      "✔ Export started for parkesia_motacilla subset 7\n",
      "\n",
      "--- Processing: geothlypis_formosa ---\n",
      "Reading parquet\n",
      "1568\n",
      "✔ Loaded Parquet data for geothlypis_formosa\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for geothlypis_formosa subset 2\n",
      "Setting up exports\n",
      "✔ Export started for geothlypis_formosa subset 0\n",
      "Setting up exports\n",
      "✔ Export started for geothlypis_formosa subset 4\n",
      "Setting up exports\n",
      "✔ Export started for geothlypis_formosa subset 1\n",
      "Setting up exports\n",
      "✔ Export started for geothlypis_formosa subset 3\n",
      "Setting up exports\n",
      "✔ Export started for geothlypis_formosa subset 5\n",
      "✔ Export started for geothlypis_formosa subset 6\n",
      "✔ Export started for geothlypis_formosa subset 8\n",
      "✔ Export started for geothlypis_formosa subset 7\n",
      "✔ Export started for geothlypis_formosa subset 9\n",
      "\n",
      "--- Processing: archilochus_colubris ---\n",
      "Reading parquet\n",
      "6828\n",
      "✔ Loaded Parquet data for archilochus_colubris\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for archilochus_colubris subset 0\n",
      "Setting up exports\n",
      "✔ Export started for archilochus_colubris subset 1\n",
      "Setting up exports\n",
      "✔ Export started for archilochus_colubris subset 3\n",
      "Setting up exports\n",
      "✔ Export started for archilochus_colubris subset 4\n",
      "Setting up exports\n",
      "✔ Export started for archilochus_colubris subset 2\n",
      "Setting up exports\n",
      "✔ Export started for archilochus_colubris subset 5\n",
      "✔ Export started for archilochus_colubris subset 6\n",
      "✔ Export started for archilochus_colubris subset 8\n",
      "✔ Export started for archilochus_colubris subset 7\n",
      "✔ Export started for archilochus_colubris subset 9\n",
      "\n",
      "--- Processing: elanoides_forficatus ---\n",
      "Reading parquet\n",
      "588\n",
      "✔ Loaded Parquet data for elanoides_forficatus\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for elanoides_forficatus subset 2\n",
      "Setting up exports\n",
      "✔ Export started for elanoides_forficatus subset 0\n",
      "Setting up exports\n",
      "✔ Export started for elanoides_forficatus subset 1\n",
      "Setting up exports\n",
      "✔ Export started for elanoides_forficatus subset 3\n",
      "Setting up exports\n",
      "✔ Export started for elanoides_forficatus subset 4\n",
      "Setting up exports\n",
      "✔ Export started for elanoides_forficatus subset 5\n",
      "✔ Export started for elanoides_forficatus subset 6\n",
      "✔ Export started for elanoides_forficatus subset 7\n",
      "✔ Export started for elanoides_forficatus subset 8\n",
      "✔ Export started for elanoides_forficatus subset 9\n",
      "\n",
      "--- Processing: vireo_flavifrons ---\n",
      "Reading parquet\n",
      "2481\n",
      "✔ Loaded Parquet data for vireo_flavifrons\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for vireo_flavifrons subset 0\n",
      "Setting up exports\n",
      "✔ Export started for vireo_flavifrons subset 1\n",
      "Setting up exports\n",
      "✔ Export started for vireo_flavifrons subset 4\n",
      "Setting up exports\n",
      "✔ Export started for vireo_flavifrons subset 3\n",
      "Setting up exports\n",
      "✔ Export started for vireo_flavifrons subset 2\n",
      "Setting up exports\n",
      "✔ Export started for vireo_flavifrons subset 5\n",
      "✔ Export started for vireo_flavifrons subset 8\n",
      "✔ Export started for vireo_flavifrons subset 7\n",
      "✔ Export started for vireo_flavifrons subset 6\n",
      "✔ Export started for vireo_flavifrons subset 9\n",
      "\n",
      "--- Processing: buteo_lineatus ---\n",
      "Reading parquet\n",
      "9535\n",
      "✔ Loaded Parquet data for buteo_lineatus\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for buteo_lineatus subset 0\n",
      "Setting up exports\n",
      "✔ Export started for buteo_lineatus subset 2\n",
      "Setting up exports\n",
      "✔ Export started for buteo_lineatus subset 3\n",
      "Setting up exports\n",
      "✔ Export started for buteo_lineatus subset 1\n",
      "Setting up exports\n",
      "✔ Export started for buteo_lineatus subset 4\n",
      "Setting up exports\n",
      "✔ Export started for buteo_lineatus subset 5\n",
      "✔ Export started for buteo_lineatus subset 6\n",
      "✔ Export started for buteo_lineatus subset 7\n",
      "✔ Export started for buteo_lineatus subset 9\n",
      "✔ Export started for buteo_lineatus subset 8\n",
      "\n",
      "--- Processing: setophaga_dominica ---\n",
      "Reading parquet\n",
      "2082\n",
      "✔ Loaded Parquet data for setophaga_dominica\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for setophaga_dominica subset 2\n",
      "Setting up exports\n",
      "✔ Export started for setophaga_dominica subset 0\n",
      "Setting up exports\n",
      "✔ Export started for setophaga_dominica subset 3\n",
      "Setting up exports\n",
      "✔ Export started for setophaga_dominica subset 1\n",
      "Setting up exports\n",
      "✔ Export started for setophaga_dominica subset 4\n",
      "Setting up exports\n",
      "✔ Export started for setophaga_dominica subset 5\n",
      "✔ Export started for setophaga_dominica subset 7\n",
      "✔ Export started for setophaga_dominica subset 6\n",
      "✔ Export started for setophaga_dominica subset 8\n",
      "✔ Export started for setophaga_dominica subset 9\n",
      "\n",
      "--- Processing: setophaga_citrina ---\n",
      "Reading parquet\n",
      "1602\n",
      "✔ Loaded Parquet data for setophaga_citrina\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for setophaga_citrina subset 1\n",
      "Setting up exports\n",
      "✔ Export started for setophaga_citrina subset 4\n",
      "Setting up exports\n",
      "✔ Export started for setophaga_citrina subset 2\n",
      "Setting up exports\n",
      "✔ Export started for setophaga_citrina subset 0\n",
      "Setting up exports\n",
      "✔ Export started for setophaga_citrina subset 3\n",
      "Setting up exports\n",
      "✔ Export started for setophaga_citrina subset 5\n",
      "✔ Export started for setophaga_citrina subset 6\n",
      "✔ Export started for setophaga_citrina subset 7\n",
      "✔ Export started for setophaga_citrina subset 8\n",
      "✔ Export started for setophaga_citrina subset 9\n",
      "\n",
      "--- Processing: dryocopus_pileatus ---\n",
      "Reading parquet\n",
      "8069\n",
      "✔ Loaded Parquet data for dryocopus_pileatus\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for dryocopus_pileatus subset 1\n",
      "Setting up exports\n",
      "✔ Export started for dryocopus_pileatus subset 4\n",
      "Setting up exports\n",
      "✔ Export started for dryocopus_pileatus subset 0\n",
      "Setting up exports\n",
      "✔ Export started for dryocopus_pileatus subset 2\n",
      "Setting up exports\n",
      "✔ Export started for dryocopus_pileatus subset 3\n",
      "Setting up exports\n",
      "✔ Export started for dryocopus_pileatus subset 5\n",
      "✔ Export started for dryocopus_pileatus subset 6\n",
      "✔ Export started for dryocopus_pileatus subset 7\n",
      "✔ Export started for dryocopus_pileatus subset 8\n",
      "✔ Export started for dryocopus_pileatus subset 9\n",
      "\n",
      "--- Processing: meleagris_gallopavo ---\n",
      "Reading parquet\n",
      "1205\n",
      "✔ Loaded Parquet data for meleagris_gallopavo\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for meleagris_gallopavo subset 4\n",
      "Setting up exports\n",
      "✔ Export started for meleagris_gallopavo subset 3\n",
      "Setting up exports\n",
      "✔ Export started for meleagris_gallopavo subset 0\n",
      "Setting up exports\n",
      "✔ Export started for meleagris_gallopavo subset 2\n",
      "Setting up exports\n",
      "✔ Export started for meleagris_gallopavo subset 1\n",
      "Setting up exports\n",
      "✔ Export started for meleagris_gallopavo subset 5\n",
      "✔ Export started for meleagris_gallopavo subset 6\n",
      "✔ Export started for meleagris_gallopavo subset 7\n",
      "✔ Export started for meleagris_gallopavo subset 8\n",
      "✔ Export started for meleagris_gallopavo subset 9\n",
      "\n",
      "--- Processing: sphyrapicus_varius ---\n",
      "Reading parquet\n",
      "2760\n",
      "✔ Loaded Parquet data for sphyrapicus_varius\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for sphyrapicus_varius subset 2\n",
      "Setting up exports\n",
      "✔ Export started for sphyrapicus_varius subset 1\n",
      "Setting up exports\n",
      "✔ Export started for sphyrapicus_varius subset 4\n",
      "Setting up exports\n",
      "✔ Export started for sphyrapicus_varius subset 3\n",
      "Setting up exports\n",
      "✔ Export started for sphyrapicus_varius subset 0\n",
      "Setting up exports\n",
      "✔ Export started for sphyrapicus_varius subset 5\n",
      "✔ Export started for sphyrapicus_varius subset 6\n",
      "✔ Export started for sphyrapicus_varius subset 9\n",
      "✔ Export started for sphyrapicus_varius subset 7\n",
      "✔ Export started for sphyrapicus_varius subset 8\n",
      "\n",
      "--- Processing: odocoileus_virginianus ---\n",
      "Reading csv\n",
      "✔ Loaded CSV for odocoileus_virginianus\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for odocoileus_virginianus subset 1\n",
      "Setting up exports\n",
      "✔ Export started for odocoileus_virginianus subset 2\n",
      "Setting up exports\n",
      "✔ Export started for odocoileus_virginianus subset 4\n",
      "Setting up exports\n",
      "✔ Export started for odocoileus_virginianus subset 3\n",
      "Setting up exports\n",
      "✔ Export started for odocoileus_virginianus subset 0\n",
      "Setting up exports\n",
      "✔ Export started for odocoileus_virginianus subset 5\n",
      "✔ Export started for odocoileus_virginianus subset 6\n",
      "✔ Export started for odocoileus_virginianus subset 7\n",
      "✔ Export started for odocoileus_virginianus subset 8\n",
      "✔ Export started for odocoileus_virginianus subset 9\n",
      "\n",
      "--- Processing: ursus_americanus ---\n",
      "Reading csv\n",
      "✔ Loaded CSV for ursus_americanus\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2024/1961102779.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for ursus_americanus subset 3\n",
      "✔ Export started for ursus_americanus subset 4\n",
      "✔ Export started for ursus_americanus subset 0\n",
      "✔ Export started for ursus_americanus subset 2\n",
      "✔ Export started for ursus_americanus subset 1\n",
      "\n",
      "--- Processing: anaxyrus_americanus ---\n",
      "Reading csv\n",
      "✔ Loaded CSV for anaxyrus_americanus\n",
      "converting to fc\n",
      "splitting fc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2024/1961102779.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for anaxyrus_americanus subset 0\n",
      "Setting up exports\n",
      "✔ Export started for anaxyrus_americanus subset 4\n",
      "Setting up exports\n",
      "✔ Export started for anaxyrus_americanus subset 3\n",
      "Setting up exports\n",
      "✔ Export started for anaxyrus_americanus subset 2\n",
      "Setting up exports\n",
      "✔ Export started for anaxyrus_americanus subset 1\n",
      "✔ Export started for anaxyrus_americanus subset 5\n",
      "✔ Export started for anaxyrus_americanus subset 6\n",
      "✔ Export started for anaxyrus_americanus subset 7\n",
      "✔ Export started for anaxyrus_americanus subset 8\n",
      "\n",
      "--- Processing: anaxyrus_fowleri ---\n",
      "Reading csv\n",
      "✔ Loaded CSV for anaxyrus_fowleri\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for anaxyrus_fowleri subset 0\n",
      "Setting up exports\n",
      "✔ Export started for anaxyrus_fowleri subset 4\n",
      "Setting up exports\n",
      "✔ Export started for anaxyrus_fowleri subset 3\n",
      "Setting up exports\n",
      "✔ Export started for anaxyrus_fowleri subset 2\n",
      "Setting up exports\n",
      "✔ Export started for anaxyrus_fowleri subset 1\n",
      "Setting up exports\n",
      "✔ Export started for anaxyrus_fowleri subset 5\n",
      "✔ Export started for anaxyrus_fowleri subset 6\n",
      "✔ Export started for anaxyrus_fowleri subset 7\n",
      "✔ Export started for anaxyrus_fowleri subset 8\n",
      "✔ Export started for anaxyrus_fowleri subset 9\n",
      "\n",
      "--- Processing: gastrophryne_carolinensis ---\n",
      "Reading csv\n",
      "✔ Loaded CSV for gastrophryne_carolinensis\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for gastrophryne_carolinensis subset 0\n",
      "Setting up exports\n",
      "✔ Export started for gastrophryne_carolinensis subset 2\n",
      "Setting up exports\n",
      "✔ Export started for gastrophryne_carolinensis subset 4\n",
      "Setting up exports\n",
      "✔ Export started for gastrophryne_carolinensis subset 3\n",
      "Setting up exports\n",
      "✔ Export started for gastrophryne_carolinensis subset 1\n",
      "Setting up exports\n",
      "✔ Export started for gastrophryne_carolinensis subset 5\n",
      "✔ Export started for gastrophryne_carolinensis subset 6\n",
      "✔ Export started for gastrophryne_carolinensis subset 7\n",
      "✔ Export started for gastrophryne_carolinensis subset 8\n",
      "✔ Export started for gastrophryne_carolinensis subset 9\n",
      "\n",
      "--- Processing: hyla_avivoca ---\n",
      "Reading csv\n",
      "✔ Loaded CSV for hyla_avivoca\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for hyla_avivoca subset 2\n",
      "Setting up exports\n",
      "✔ Export started for hyla_avivoca subset 0\n",
      "Setting up exports\n",
      "✔ Export started for hyla_avivoca subset 1\n",
      "✔ Export started for hyla_avivoca subset 4\n",
      "✔ Export started for hyla_avivoca subset 3\n",
      "✔ Export started for hyla_avivoca subset 6\n",
      "✔ Export started for hyla_avivoca subset 5\n",
      "\n",
      "--- Processing: hyla_chrysoscelis ---\n",
      "Reading csv\n",
      "✔ Loaded CSV for hyla_chrysoscelis\n",
      "converting to fc\n",
      "splitting fc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2024/1961102779.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for hyla_chrysoscelis subset 0\n",
      "Setting up exports\n",
      "✔ Export started for hyla_chrysoscelis subset 1\n",
      "Setting up exports\n",
      "✔ Export started for hyla_chrysoscelis subset 2\n",
      "Setting up exports\n",
      "✔ Export started for hyla_chrysoscelis subset 3\n",
      "Setting up exports\n",
      "✔ Export started for hyla_chrysoscelis subset 4\n",
      "Setting up exports\n",
      "✔ Export started for hyla_chrysoscelis subset 5\n",
      "✔ Export started for hyla_chrysoscelis subset 6\n",
      "✔ Export started for hyla_chrysoscelis subset 7\n",
      "✔ Export started for hyla_chrysoscelis subset 8\n",
      "✔ Export started for hyla_chrysoscelis subset 9\n",
      "\n",
      "--- Processing: hyla_cinerea ---\n",
      "Reading csv\n",
      "✔ Loaded CSV for hyla_cinerea\n",
      "converting to fc\n",
      "splitting fc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2024/1961102779.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for hyla_cinerea subset 2\n",
      "Setting up exports\n",
      "✔ Export started for hyla_cinerea subset 1\n",
      "Setting up exports\n",
      "✔ Export started for hyla_cinerea subset 4\n",
      "Setting up exports\n",
      "✔ Export started for hyla_cinerea subset 0\n",
      "Setting up exports\n",
      "✔ Export started for hyla_cinerea subset 3\n",
      "Setting up exports\n",
      "✔ Export started for hyla_cinerea subset 5\n",
      "✔ Export started for hyla_cinerea subset 6\n",
      "✔ Export started for hyla_cinerea subset 7\n",
      "✔ Export started for hyla_cinerea subset 9\n",
      "✔ Export started for hyla_cinerea subset 8\n",
      "\n",
      "--- Processing: hyla_squirella ---\n",
      "Reading csv\n",
      "✔ Loaded CSV for hyla_squirella\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for hyla_squirella subset 4\n",
      "Setting up exports\n",
      "✔ Export started for hyla_squirella subset 0\n",
      "Setting up exports\n",
      "✔ Export started for hyla_squirella subset 1\n",
      "Setting up exports\n",
      "✔ Export started for hyla_squirella subset 3\n",
      "Setting up exports\n",
      "✔ Export started for hyla_squirella subset 2\n",
      "Setting up exports\n",
      "✔ Export started for hyla_squirella subset 5\n",
      "✔ Export started for hyla_squirella subset 6\n",
      "✔ Export started for hyla_squirella subset 9\n",
      "✔ Export started for hyla_squirella subset 8\n",
      "✔ Export started for hyla_squirella subset 7\n",
      "\n",
      "--- Processing: hyla_versicolor ---\n",
      "Reading csv\n",
      "✔ Loaded CSV for hyla_versicolor\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for hyla_versicolor subset 2\n",
      "✔ Export started for hyla_versicolor subset 1\n",
      "✔ Export started for hyla_versicolor subset 0\n",
      "✔ Export started for hyla_versicolor subset 3\n",
      "\n",
      "--- Processing: lithobates_catesbeianus ---\n",
      "Reading csv\n",
      "✔ Loaded CSV for lithobates_catesbeianus\n",
      "converting to fc\n",
      "splitting fc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2024/1961102779.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for lithobates_catesbeianus subset 4\n",
      "Setting up exports\n",
      "✔ Export started for lithobates_catesbeianus subset 3\n",
      "Setting up exports\n",
      "✔ Export started for lithobates_catesbeianus subset 2\n",
      "Setting up exports\n",
      "✔ Export started for lithobates_catesbeianus subset 0\n",
      "Setting up exports\n",
      "✔ Export started for lithobates_catesbeianus subset 1\n",
      "Setting up exports\n",
      "✔ Export started for lithobates_catesbeianus subset 5\n",
      "✔ Export started for lithobates_catesbeianus subset 6\n",
      "✔ Export started for lithobates_catesbeianus subset 7\n",
      "✔ Export started for lithobates_catesbeianus subset 8\n",
      "✔ Export started for lithobates_catesbeianus subset 9\n",
      "\n",
      "--- Processing: lithobates_clamitans ---\n",
      "Reading csv\n",
      "✔ Loaded CSV for lithobates_clamitans\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for lithobates_clamitans subset 0\n",
      "Setting up exports\n",
      "✔ Export started for lithobates_clamitans subset 4\n",
      "Setting up exports\n",
      "✔ Export started for lithobates_clamitans subset 2\n",
      "Setting up exports\n",
      "✔ Export started for lithobates_clamitans subset 1\n",
      "Setting up exports\n",
      "✔ Export started for lithobates_clamitans subset 3\n",
      "Setting up exports\n",
      "✔ Export started for lithobates_clamitans subset 5\n",
      "✔ Export started for lithobates_clamitans subset 6\n",
      "✔ Export started for lithobates_clamitans subset 7\n",
      "✔ Export started for lithobates_clamitans subset 8\n",
      "✔ Export started for lithobates_clamitans subset 9\n",
      "\n",
      "--- Processing: lithobates_palustris ---\n",
      "Reading csv\n",
      "✔ Loaded CSV for lithobates_palustris\n",
      "❌ No records in date range for lithobates_palustris\n",
      "❌ No data found for lithobates_palustris\n",
      "\n",
      "--- Processing: lithobates_sphenocephalus ---\n",
      "Reading csv\n",
      "✔ Loaded CSV for lithobates_sphenocephalus\n",
      "converting to fc\n",
      "splitting fc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2024/1961102779.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for lithobates_sphenocephalus subset 1\n",
      "Setting up exports\n",
      "✔ Export started for lithobates_sphenocephalus subset 0\n",
      "Setting up exports\n",
      "✔ Export started for lithobates_sphenocephalus subset 2\n",
      "Setting up exports\n",
      "✔ Export started for lithobates_sphenocephalus subset 4\n",
      "Setting up exports\n",
      "✔ Export started for lithobates_sphenocephalus subset 3\n",
      "Setting up exports\n",
      "✔ Export started for lithobates_sphenocephalus subset 6\n",
      "✔ Export started for lithobates_sphenocephalus subset 8\n",
      "✔ Export started for lithobates_sphenocephalus subset 5\n",
      "✔ Export started for lithobates_sphenocephalus subset 9\n",
      "✔ Export started for lithobates_sphenocephalus subset 7\n",
      "\n",
      "--- Processing: pseudacris_crucifer ---\n",
      "Reading csv\n",
      "✔ Loaded CSV for pseudacris_crucifer\n",
      "converting to fc\n",
      "splitting fc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2024/1961102779.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for pseudacris_crucifer subset 3\n",
      "Setting up exports\n",
      "✔ Export started for pseudacris_crucifer subset 1\n",
      "Setting up exports\n",
      "✔ Export started for pseudacris_crucifer subset 0\n",
      "Setting up exports\n",
      "✔ Export started for pseudacris_crucifer subset 2\n",
      "Setting up exports\n",
      "✔ Export started for pseudacris_crucifer subset 4\n",
      "✔ Export started for pseudacris_crucifer subset 5\n",
      "✔ Export started for pseudacris_crucifer subset 6\n",
      "✔ Export started for pseudacris_crucifer subset 8\n",
      "✔ Export started for pseudacris_crucifer subset 7\n",
      "\n",
      "--- Processing: pseudacris_fouquettei ---\n",
      "Reading csv\n",
      "✔ Loaded CSV for pseudacris_fouquettei\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for pseudacris_fouquettei subset 2\n",
      "Setting up exports\n",
      "✔ Export started for pseudacris_fouquettei subset 1\n",
      "Setting up exports\n",
      "✔ Export started for pseudacris_fouquettei subset 0\n",
      "Setting up exports\n",
      "✔ Export started for pseudacris_fouquettei subset 3\n",
      "Setting up exports\n",
      "✔ Export started for pseudacris_fouquettei subset 4\n",
      "✔ Export started for pseudacris_fouquettei subset 5\n",
      "✔ Export started for pseudacris_fouquettei subset 6\n",
      "✔ Export started for pseudacris_fouquettei subset 7\n",
      "✔ Export started for pseudacris_fouquettei subset 8\n",
      "\n",
      "--- Processing: kinosternon_subrubrum ---\n",
      "Reading csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2024/1961102779.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Loaded CSV for kinosternon_subrubrum\n",
      "converting to fc\n",
      "splitting fc\n",
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for kinosternon_subrubrum subset 0\n",
      "Setting up exports\n",
      "✔ Export started for kinosternon_subrubrum subset 2\n",
      "Setting up exports\n",
      "✔ Export started for kinosternon_subrubrum subset 3\n",
      "Setting up exports\n",
      "✔ Export started for kinosternon_subrubrum subset 1\n",
      "Setting up exports\n",
      "✔ Export started for kinosternon_subrubrum subset 4\n",
      "✔ Export started for kinosternon_subrubrum subset 5\n",
      "✔ Export started for kinosternon_subrubrum subset 6\n",
      "✔ Export started for kinosternon_subrubrum subset 7\n",
      "✔ Export started for kinosternon_subrubrum subset 8\n",
      "\n",
      "--- Processing: apalone_spinifera ---\n",
      "Reading csv\n",
      "✔ Loaded CSV for apalone_spinifera\n",
      "converting to fc\n",
      "splitting fc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2024/1961102779.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "Setting up exports\n",
      "✔ Export started for apalone_spinifera subset 0\n",
      "Setting up exports\n",
      "✔ Export started for apalone_spinifera subset 4\n",
      "Setting up exports\n",
      "✔ Export started for apalone_spinifera subset 2\n",
      "Setting up exports\n",
      "✔ Export started for apalone_spinifera subset 1\n",
      "Setting up exports\n",
      "✔ Export started for apalone_spinifera subset 5\n",
      "✔ Export started for apalone_spinifera subset 3\n",
      "✔ Export started for apalone_spinifera subset 7\n",
      "✔ Export started for apalone_spinifera subset 6\n",
      "✔ Export started for apalone_spinifera subset 8\n",
      "\n",
      "--- Processing: macrochelys_temmincki ---\n",
      "Reading parquet\n",
      "⚠️ DuckDB query failed for macrochelys_temmincki: IO Error: No files found that match the pattern \"/mnt/c/ebirdpolars/scientific_name=macrochelys_temmincki/*.parquet\"\n",
      "\n",
      "LINE 9:                     FROM read_parquet('/mnt/c/ebirdpolars/scientific_name=macrochely...\n",
      "                                 ^\n",
      "❌ No data found for macrochelys_temmincki\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# MAIN LOOP\n",
    "# -------------------------\n",
    "con = duckdb.connect()\n",
    "con.sql('INSTALL spatial; LOAD spatial')\n",
    "con.execute(f\"\"\"\n",
    "CREATE OR REPLACE TABLE aoi AS\n",
    "SELECT geometry, ST_Transform(geometry, 'EPSG:5070', 'EPSG:4326', always_xy := true) AS t_geom\n",
    "FROM read_parquet('{aoifile}');\n",
    "\"\"\")\n",
    "\n",
    "for species in species_list:\n",
    "    try:\n",
    "        log(f\"\\n--- Processing: {species} ---\")\n",
    "        df = load_species_data(species)\n",
    "        if df is None or df.empty:\n",
    "            log(f\"❌ No data found for {species}\")\n",
    "            continue\n",
    "\n",
    "        training_fc = df_to_ee_fc(df, 'date')\n",
    "        subsets = split_fc(training_fc, n_subsets=10)\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            for i, sub_fc in enumerate(subsets):\n",
    "                executor.submit(export_subset, sub_fc, species, i)\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"❌ Failed to process {species}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb144aab-108d-4b50-9cea-27d2665d7225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
