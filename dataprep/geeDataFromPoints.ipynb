{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a89537-3292-4108-a199-9372aecf4036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ee\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "ee.Authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1509d72-e703-44ef-a91d-e660bb8a8748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee, os, pandas as pd, duckdb\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "\n",
    "# Authenticate & initialize Earth Engine\n",
    "ee.Initialize(project='ee-mkmitchellducks')\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "csv_folder = \"/mnt/c/gbif\"\n",
    "parquet_folder = \"/mnt/c/ebirdpolars\"\n",
    "\n",
    "asset_folder = \"projects/ee-mkmitchellducks/assets/gbif\"\n",
    "aoifile = '/mnt/e/gis/BaseData/MAV_Boundary_4326_wkb.parquet'\n",
    "\n",
    "scale = 100  # Adjust based on your imagery resolution\n",
    "aoi_gdf = gpd.read_parquet(aoifile)\n",
    "aoi_geom = ee.Geometry.Polygon(list(aoi_gdf.geometry.union_all().exterior.coords))\n",
    "# Buffer by 5000 m for focal stats\n",
    "aoi_buffered = aoi_geom.buffer(5000)  # 5 km buffer\n",
    "\n",
    "species_list = [\n",
    "    \"Protonotaria citrea\",\n",
    "    \"Limnothlypis swainsonii\",\n",
    "    \"Setophaga americana\",\n",
    "    \"Empidonax virescens\",\n",
    "    \"Coccyzus americanus\",\n",
    "    \"Vireo griseus\",\n",
    "    \"Setophaga cerulea\",\n",
    "    \"Hylocichla mustelina\",\n",
    "    \"Parkesia motacilla\",\n",
    "    \"Geothlypis formosa\",\n",
    "    \"Archilochus colubris\",\n",
    "    \"Elanoides forficatus\",\n",
    "    \"Vireo flavifrons\",\n",
    "    \"Buteo lineatus\",\n",
    "    \"Setophaga dominica\",\n",
    "    \"Setophaga citrina\",\n",
    "    \"Dryocopus pileatus\",\n",
    "    \"Meleagris gallopavo\",\n",
    "    \"Sphyrapicus varius\",\n",
    "    #\"Odocoileus virginianus\",     # white tailed deer\n",
    "    #\"Ursus americanus\",           # Black bear\n",
    "    #\"Anaxyrus americanus\",        # American Toad\n",
    "    #\"Anaxyrus fowleri\",           # Fowler's Toad\n",
    "    #\"Gastrophryne carolinensis\",  # Eastern Narrow-mouthed Toad\n",
    "    #\"Hyla avivoca\",               # Bird-voiced Treefrog\n",
    "    #\"Hyla chrysoscelis\",          # Cope's Gray Treefrog\n",
    "    #\"Hyla cinerea\",               # Green Treefrog\n",
    "    #\"Hyla squirella\",             # Squirrel Treefrog    \n",
    "    #\"Hyla versicolor\",            # Gray Treefrog\n",
    "    #\"Lithobates catesbeianus\",    # American Bullfrog\n",
    "    #\"Lithobates clamitans\",       # Bronze Frog\n",
    "    #\"Lithobates palustris\",       # Pickerel Frog\n",
    "    #\"Lithobates sphenocephalus\",  # Southern Leopard Frog\n",
    "    #\"Pseudacris crucifer\",        # Spring Peeper\n",
    "    #\"Pseudacris fouquettei\",      # Cajun Chorus Frog\n",
    "    #\"Kinosternon subrubrum\",      # Eastern Mud Turtle\n",
    "    #\"Apalone spinifera\",          # Spiny Softshell Turtle   \n",
    "    #\"Macrochelys temmincki\"       # Alligator Snapping Turtle    \n",
    "    ]\n",
    "species_list = [item.strip().lower().replace(' ', '_') for item in species_list]\n",
    "log_file = \"process_log.txt\"\n",
    "\n",
    "# -------------------------\n",
    "# Logging\n",
    "# -------------------------\n",
    "def log(message):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(f\"[{timestamp}] {message}\\n\")\n",
    "    print(message)\n",
    "\n",
    "# -------------------------\n",
    "# Load species data from CSV or DuckDB\n",
    "# -------------------------\n",
    "def load_species_data(species):\n",
    "    csv_files = glob.glob(os.path.join(csv_folder, f\"{species}*.csv\"))\n",
    "    df = None\n",
    "    if len(csv_files)>0:\n",
    "        print('Reading csv')\n",
    "        try:\n",
    "            # Read and combine all CSVs\n",
    "            df_list = [pd.read_csv(f) for f in csv_files]\n",
    "            df = pd.concat(df_list, ignore_index=True)\n",
    "            #df['date'] = pd.to_datetime(df['eventdate'])\n",
    "            df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "            df = df[['basisofrecord', 'species', 'latitude', 'longitude','coordinateuncertaintyinmeters', 'date']]\n",
    "            df = df.dropna(how='any')\n",
    "            df = df[df['coordinateuncertaintyinmeters'] <=100]\n",
    "            log(f\"✔ Loaded CSV for {species}\")\n",
    "        except Exception as e:\n",
    "            log(f\"⚠️ Failed to read CSV for {species}: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print('Reading parquet')\n",
    "        try:\n",
    "            query = f\"\"\"\n",
    "                    SELECT \n",
    "                        lower(eb.scientific_name) as scientific_name,\n",
    "                        eb.observation_date,\n",
    "                        eb.protocol_name,\n",
    "                        eb.effort_distance_km,\n",
    "                        eb.longitude,\n",
    "                        eb.latitude\n",
    "                    FROM read_parquet('{parquet_folder}/scientific_name={species}/*.parquet', hive_partitioning = true) AS eb\n",
    "                    JOIN aoi\n",
    "                        ON ST_Intersects(ST_Point(eb.longitude, eb.latitude), aoi.geometry)\n",
    "                    WHERE lower(eb.scientific_name) = '{species}'\n",
    "                      AND CAST(substr(eb.observation_date, 1, 4) AS INTEGER) BETWEEN 2017 AND 2024;  \n",
    "            \"\"\"\n",
    "            df = con.execute(query).fetchdf()\n",
    "            print(len(df))\n",
    "            df = df[(df['effort_distance_km']<.1) | (df['protocol_name'] == 'Stationary')]\n",
    "            df = df .drop(columns=['effort_distance_km'])\n",
    "            df['date'] = pd.to_datetime(df['observation_date'])\n",
    "            # Extract year, month, day\n",
    "            df['year'] = df['date'].dt.year\n",
    "            df['month'] = df['date'].dt.month\n",
    "            df['day'] = df['date'].dt.day   \n",
    "            if df.empty:\n",
    "                log(f\"❌ No matching records in Parquet for {species}\")\n",
    "                return None\n",
    "            log(f\"✔ Loaded Parquet data for {species}\")\n",
    "        except Exception as e:\n",
    "            log(f\"⚠️ DuckDB query failed for {species}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Filter by date range\n",
    "    try:\n",
    "        if df.empty:\n",
    "            log(f\"❌ No records in date range for {species}\")\n",
    "            return None\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        log(f\"⚠️ Failed to filter dates for {species}: {e}\")\n",
    "        return None\n",
    "\n",
    "# -------------------------\n",
    "# Convert DataFrame to EE FeatureCollection\n",
    "# -------------------------\n",
    "def df_to_ee_fc(df, datefield, lon_col='longitude', lat_col='latitude', properties=None):\n",
    "    df = df.dropna(subset=[lon_col, lat_col])\n",
    "    print('converting to fc')\n",
    "    if properties is None:\n",
    "        properties = [c for c in df.columns if c not in [lon_col, lat_col]]\n",
    "    features = []\n",
    "    for _, row in df.iterrows():\n",
    "        geom = ee.Geometry.Point([row[lon_col], row[lat_col]])\n",
    "        props = {k: row[k] for k in properties}\n",
    "        props['obs_date'] = row[datefield].strftime('%Y-%m-%d')\n",
    "        features.append(ee.Feature(geom, props))\n",
    "    return ee.FeatureCollection(features)\n",
    "    \n",
    "# -------------------------\n",
    "# Split FeatureCollection into subsets\n",
    "# -------------------------\n",
    "def split_fc(fc, n_subsets=10):\n",
    "    print('splitting fc')\n",
    "    n_points = fc.size().getInfo()\n",
    "    points_list = fc.toList(n_points)\n",
    "    subsets = []\n",
    "    step = n_points // n_subsets + 1\n",
    "    for i in range(0, n_points, step):\n",
    "        subset = ee.FeatureCollection(points_list.slice(i, i + step))\n",
    "        subsets.append(subset)\n",
    "    print('split')\n",
    "    return subsets\n",
    "\n",
    "# -------------------------\n",
    "# Dynamic World mode image\n",
    "# -------------------------\n",
    "def get_dw_mode_image(obs_date):\n",
    "    obs_date = ee.Date(obs_date)\n",
    "    start_date = obs_date.advance(-3, 'month')\n",
    "    dw_collection = ee.ImageCollection(\"GOOGLE/DYNAMICWORLD/V1\").select(\"label\")\n",
    "    dw_filtered = dw_collection.filterDate(start_date, obs_date)\n",
    "    return dw_filtered.reduce(ee.Reducer.mode())\n",
    "\n",
    "# -------------------------\n",
    "# Percent cover for DW classes\n",
    "# -------------------------\n",
    "def compute_dw_percent_cover(dw_img, radius_m):\n",
    "    class_ids = list(range(9))\n",
    "    kernel = ee.Kernel.circle(radius=radius_m, units='meters', normalize=True)\n",
    "    cover_images = []\n",
    "    for class_id in class_ids:\n",
    "        mask = dw_img.eq(class_id)\n",
    "        pct = mask.reduceNeighborhood(ee.Reducer.mean(), kernel).multiply(100).rename(f'dw_class_{class_id}_pct_{radius_m}m')\n",
    "        cover_images.append(pct)\n",
    "    return ee.Image.cat(cover_images)\n",
    "\n",
    "# -------------------------\n",
    "# Forest edge and core\n",
    "# -------------------------\n",
    "def compute_forest_metrics(dw_img, radius_m):\n",
    "    forest_mask = dw_img.eq(1)\n",
    "    non_forest_mask = dw_img.neq(1)\n",
    "\n",
    "    forest_edges = ee.Algorithms.CannyEdgeDetector(image=forest_mask, threshold=0.5, sigma=1)\n",
    "    edge_density = forest_edges.reduceNeighborhood(\n",
    "        ee.Reducer.sum(), ee.Kernel.circle(radius=radius_m, units='meters')\n",
    "    ).rename('forest_edge_length')\n",
    "\n",
    "    non_forest_buffer = non_forest_mask.focal_max(radius=100, units='meters')\n",
    "    forest_core = forest_mask.And(non_forest_buffer.Not()).rename('forest_core')\n",
    "\n",
    "    return ee.Image.cat([edge_density, forest_core])\n",
    "\n",
    "# -------------------------\n",
    "# Export Task Function\n",
    "# -------------------------\n",
    "def export_subset(sub_fc, species, subset_index):\n",
    "    print('Setting up exports')\n",
    "    def process_feature(f):\n",
    "        obs_date = f.get('obs_date')\n",
    "        dw_img = get_dw_mode_image(obs_date)\n",
    "\n",
    "        cover_100m = compute_dw_percent_cover(dw_img, radius_m=100)\n",
    "        cover_10km = compute_dw_percent_cover(dw_img, radius_m=10000)\n",
    "        forest_metrics_100m = compute_forest_metrics(dw_img, 100)\n",
    "        forest_metrics_10km = compute_forest_metrics(dw_img, 10000)\n",
    "\n",
    "        full_img = ee.Image.cat([cover_100m, cover_10km, forest_metrics_100m, forest_metrics_10km])\n",
    "        sampled = full_img.sampleRegions(\n",
    "            collection=ee.FeatureCollection([f]),\n",
    "            scale=100,\n",
    "            geometries=True,\n",
    "            tileScale=4\n",
    "        )\n",
    "        return sampled\n",
    "\n",
    "    try:\n",
    "        sampled_fc = sub_fc.map(process_feature).flatten()\n",
    "        export_desc = f\"{species}_subset{subset_index}\"\n",
    "        asset_id = f\"{asset_folder}/{export_desc}\"\n",
    "\n",
    "        task = ee.batch.Export.table.toDrive(\n",
    "            collection=sampled_fc,\n",
    "            description=export_desc,\n",
    "            fileNamePrefix=asset_id\n",
    "        )\n",
    "        task.start()\n",
    "        log(f\"✔ Export started for {species} subset {subset_index}\")\n",
    "    except Exception as e:\n",
    "        log(f\"❌ Export failed for {species} subset {subset_index}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc92cd19-b2c2-4f72-be4a-76a4af992eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# MAIN LOOP\n",
    "# -------------------------\n",
    "con = duckdb.connect()\n",
    "con.sql('INSTALL spatial; LOAD spatial')\n",
    "con.execute(f\"\"\"\n",
    "CREATE OR REPLACE TABLE aoi AS\n",
    "SELECT geometry, ST_Transform(geometry, 'EPSG:5070', 'EPSG:4326', always_xy := true) AS t_geom\n",
    "FROM read_parquet('{aoifile}');\n",
    "\"\"\")\n",
    "\n",
    "for species in species_list:\n",
    "    try:\n",
    "        log(f\"\\n--- Processing: {species} ---\")\n",
    "        df = load_species_data(species)\n",
    "        if df is None or df.empty:\n",
    "            log(f\"❌ No data found for {species}\")\n",
    "            continue\n",
    "\n",
    "        training_fc = df_to_ee_fc(df, 'date')\n",
    "        subsets = split_fc(training_fc, n_subsets=10)\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            for i, sub_fc in enumerate(subsets):\n",
    "                executor.submit(export_subset, sub_fc, species, i)\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"❌ Failed to process {species}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb144aab-108d-4b50-9cea-27d2665d7225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
